{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "get_ipython().system('pip install snowflake-connector-python')\n",
    "get_ipython().system('pip install tldextract')\n",
    "get_ipython().system('pip install py2neo')\n",
    "\n",
    "# In[175]:\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import snowflake.connector\n",
    "from py2neo import Graph\n",
    "from py2neo.bulk import merge_nodes, merge_relationships\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#[Environment Configs]\n",
    "client_id = \"d0afa05e7ab642acb0497f018b601b86\"\n",
    "client_secret = \"ikdU3eOtXRmMR9oWlEnP0W0jAVZYNMBF2FUwsuwtHkZ37Oh4IZZkso3tFdsbfdmO\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://d161560-001.dc.gs.com:8899\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://d161560-001.dc.gs.com:8899\"\n",
    "\n",
    "#[Credentials]\n",
    "host = \"xplore.graph-x.site.gs.com\"\n",
    "port = \"7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"xplorer\"\n",
    "graph = Graph(host=host, port=port, auth=(username, password))\n",
    "BATCHES = 100\n"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "yRnTAZpjwjntiJOe0aa4Xu",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Libraries & Shared Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_oauth_tokens(client_id: str, client_secret: str):\n",
    "    _IDFS_URL = \"https://idfs.gs.com/as/token.oauth2\"\n",
    "    response = requests.post(\n",
    "        url=_IDFS_URL,\n",
    "        params={\"access_token_manager_id\": \"JwtSnowflake\"},\n",
    "        # This must be set to JwtSnowflake to generate Snowflake-compatible OAuth Token\n",
    "        data={\"grant_type\": \"client_credentials\", \"scope\": \"SESSION:ROLE-ANY\"},\n",
    "        auth=(client_id, client_secret),\n",
    "    )\n",
    "    response = response.json()\n",
    "    if \"access_token\" not in response.keys():\n",
    "        raise PermissionError(f\"Not authorized to access OAuth Tokens: {response}\")\n",
    "    return response[\"access_token\"]\n",
    "\n",
    "\n",
    "def connect_snowflake(user: str, database: str, token: str) -> snowflake.connector.cursor:\n",
    "    \"\"\"Connect to Snowflake and return an authorized cursor. Below uses Catalyst connections\n",
    "\n",
    "    :param user: Client ID from OAuth2 Authentication\n",
    "    :param database: Name of database to connect to\n",
    "    :param token: Generated OAuth Token. Please either generate and save to a tmp file in a separate thread OR use subprocess module to call the underlying subsystem and capture the output\n",
    "    :return: Snowflake Cursor\n",
    "    \"\"\"\n",
    "    snow_connector = snowflake.connector.connect(\n",
    "        user=user,\n",
    "        authenticator=\"oauth\",\n",
    "        account=\"sfamdprvtawseast1d01.goldman.us-east-1.aws.privatelink\",\n",
    "        role=\"AMD_CATALYST_DATA_RW\",\n",
    "        warehouse=\"AMD_CATALYST_RW\",\n",
    "        database=database,\n",
    "        ocsp_fail_open=False,\n",
    "        token=token,\n",
    "    )\n",
    "    cursor = snow_connector.cursor()\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def disconnect(cursor: snowflake.connector.cursor) -> None:\n",
    "    try:\n",
    "        cursor.close()\n",
    "    except:\n",
    "        raise Exception(\"Failed to close cursor\")\n",
    "\n",
    "\n",
    "oauth_token = get_oauth_tokens(client_id, client_secret)\n",
    "cursor = connect_snowflake(client_id, \"AMD_CATALYST_DB\", oauth_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sourcescrub Person"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "df_from_database = cursor.execute('SELECT columns FROM database).fetch_pandas_all()\n",
    "\n",
    "source: database\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sourcescrub_persons_df = cursor.execute(\n",
    "    'SELECT ID, FIRSTNAME, LASTNAME, LINKEDIN, TWITTER, FACEBOOK, WEBSITE, PERSONALEMAIL, CITY, STATE, COUNTRY FROM L2.GRAPH_SS_PERSON').fetch_pandas_all()\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "\n",
    "sourcescrub_persons_df['NAME'] = sourcescrub_persons_df['FIRSTNAME'] + ' ' + sourcescrub_persons_df['LASTNAME']\n",
    "sourcescrub_persons_df.drop(columns=['FIRSTNAME', 'LASTNAME'], inplace=True)\n",
    "sourcescrub_persons_df['ID'] = 'sourcescrub-' + sourcescrub_persons_df['ID'].astype(str)\n",
    "sourcescrub_persons_df.rename(columns={'ID': 'SOURCE_ID', \"PERSONALEMAIL\": \"EMAIL\"}, inplace=True)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_persons_df['LINKEDIN'] = sourcescrub_persons_df['LINKEDIN'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.').strip('/'))\n",
    "sourcescrub_persons_df['TWITTER'] = sourcescrub_persons_df['TWITTER'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.').strip('/'))\n",
    "sourcescrub_persons_df['FACEBOOK'] = sourcescrub_persons_df['FACEBOOK'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.').strip('/'))\n",
    "sourcescrub_persons_df['WEBSITE'] = sourcescrub_persons_df['WEBSITE'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.').strip('/'))\n",
    "sourcescrub_persons_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_persons_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_persons_df_website = np.array_split(\n",
    "    sourcescrub_persons_df[sourcescrub_persons_df['WEBSITE'].isna() == False][['WEBSITE']].rename(\n",
    "        columns={'WEBSITE': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in sourcescrub_persons_df_website:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"DomainURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "sourcescrub_person_df_linkedin = np.array_split(\n",
    "    sourcescrub_persons_df[sourcescrub_persons_df['LINKEDIN'].isna() == False][['LINKEDIN']].rename(\n",
    "        columns={'LINKEDIN': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in sourcescrub_person_df_linkedin:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"LinkedInURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "sourcescrub_person_df_twitter = np.array_split(\n",
    "    sourcescrub_persons_df[sourcescrub_persons_df['TWITTER'].isna() == False][['TWITTER']].rename(\n",
    "        columns={'TWITTER': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in sourcescrub_person_df_twitter:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"TwitterURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "sourcescrub_person_df_facebook = np.array_split(\n",
    "    sourcescrub_persons_df[sourcescrub_persons_df['FACEBOOK'].isna() == False][['FACEBOOK']].rename(\n",
    "        columns={'FACEBOOK': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in sourcescrub_person_df_facebook:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"FacebookURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "sourcescrub_person_df_email = np.array_split(\n",
    "    sourcescrub_persons_df[sourcescrub_persons_df['EMAIL'].isna() == False][['EMAIL']].rename(\n",
    "        columns={'EMAIL': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in sourcescrub_person_df_email:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"EmailURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_persons = np.array_split(\n",
    "    [{k: v for k, v in x.items() if v == v} for x in sourcescrub_persons_df.to_dict('records')], BATCHES)\n",
    "for chunk_df in sourcescrub_persons:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"Person\", \"SourcescrubPerson\"), \"SOURCE_ID\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for identifier in ['WEBSITE', 'LINKEDIN', 'TWITTER', 'FACEBOOK']:\n",
    "    sourcescrub_identifier_relationships = sourcescrub_persons_df.loc[sourcescrub_persons_df[identifier] != np.nan][\n",
    "        ['SOURCE_ID', identifier]]\n",
    "    sourcescrub_identifier_relationships.insert(1, 'WEIGHT', [0.5] * len(sourcescrub_identifier_relationships))\n",
    "\n",
    "    df_list = np.array_split(sourcescrub_identifier_relationships, BATCHES)\n",
    "    for chunk_df in df_list:\n",
    "        merge_relationships(\n",
    "            graph.auto(),\n",
    "            chunk_df.values.tolist(),\n",
    "            merge_key=\"LINKED_TO\",\n",
    "            start_node_key=(\"Person\", \"SOURCE_ID\"),\n",
    "            end_node_key=(\"URLIdentifier\", \"URL\"),\n",
    "            keys=[\"WEIGHT\"],\n",
    "        )\n",
    "\n",
    "# ## PERSONTOCOMPANY\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_p2c_df = cursor.execute(\n",
    "    'SELECT COMPANYID, PERSONID, TITLE, ROLE, STARTDATE, ENDDATE FROM L1.SOURCESCRUB_PERSONTOCOMPANY').fetch_pandas_all()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_p2c_df['PERSONID'] = 'sourcescrub-' + sourcescrub_p2c_df['PERSONID'].astype(str)\n",
    "sourcescrub_p2c_df['COMPANYID'] = 'sourcescrub-' + sourcescrub_p2c_df['COMPANYID'].astype(str)\n",
    "sourcescrub_p2c_df.rename(\n",
    "    columns={'PERSONID': 'PERSON_ID', 'COMPANYID': 'COMPANY_ID', 'STARTDATE': 'START_DATE', 'ENDDATE': 'END_DATE'},\n",
    "    inplace=True)\n",
    "sourcescrub_p2c_df.START_DATE = sourcescrub_p2c_df.START_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "sourcescrub_p2c_df.END_DATE = sourcescrub_p2c_df.END_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "sourcescrub_p2c_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_p2c_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# sourcescrub_p2c_df.drop(columns = \"WEIGHT\", inplace = True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_p2c_df.insert(2, 'WEIGHT', [0.5] * len(sourcescrub_p2c_df))\n",
    "sourcescrub_p2c_df['P2C_DETAILS'] = [{k: v for k, v in x.items() if v == v} for x in\n",
    "                                     sourcescrub_p2c_df.to_dict('records')]\n",
    "\n",
    "sourcescrub_curr_emp_df = sourcescrub_p2c_df[sourcescrub_p2c_df['END_DATE'].isna() == True].loc[:,\n",
    "                          [\"COMPANY_ID\", \"P2C_DETAILS\", \"PERSON_ID\"]]\n",
    "sourcescrub_fmr_emp_df = sourcescrub_p2c_df[sourcescrub_p2c_df['END_DATE'].isna() == False].loc[:,\n",
    "                         [\"COMPANY_ID\", \"P2C_DETAILS\", \"PERSON_ID\"]]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_p2c_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sourcescrub_curr_emp_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# current employment\n",
    "sourcescrub_curr_emp = np.array_split(sourcescrub_curr_emp_df, BATCHES)\n",
    "for chunk_df in sourcescrub_curr_emp:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"CURRENT_EMPLOYMENT\",\n",
    "        start_node_key=(\"Company\", \"SOURCE_ID\"),\n",
    "        end_node_key=(\"Person\", \"SOURCE_ID\")\n",
    "    )\n",
    "# previous employment    \n",
    "sourcescrub_fmr_emp = np.array_split(sourcescrub_fmr_emp_df, BATCHES)\n",
    "for chunk_df in sourcescrub_fmr_emp:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"FORMER_EMPLOYMENT\",\n",
    "        start_node_key=(\"Company\", \"SOURCE_ID\"),\n",
    "        end_node_key=(\"Person\", \"SOURCE_ID\")\n",
    "    )\n",
    "\n",
    "# ### SourceScrub Boardmembers\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_bm_df = cursor.execute(\n",
    "    'SELECT PERSONID, COMPANYID, EMAIL, FIRSTNAME, LASTNAME, TITLE, LINKEDIN, STARTDATE, ENDDATE FROM AMD_CATALYST_DB.L2_NEW.SOURCESCRUB_BOARDMEMBERS WHERE LINKEDIN IS NOT NULL').fetch_pandas_all()\n",
    "ss_bm_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_bm_df['NAME'] = ss_bm_df['FIRSTNAME'] + ' ' + ss_bm_df['LASTNAME']\n",
    "ss_bm_df.drop(columns=['FIRSTNAME', 'LASTNAME'], inplace=True)\n",
    "ss_bm_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "ss_bm_df.rename(\n",
    "    columns={'PERSONID': 'PERSON_ID', 'COMPANYID': 'COMPANY_ID', 'STARTDATE': 'START_DATE', 'ENDDATE': 'END_DATE'},\n",
    "    inplace=True)\n",
    "ss_bm_df.START_DATE = ss_bm_df.START_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "ss_bm_df.END_DATE = ss_bm_df.END_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "ss_bm_df.insert(2, 'WEIGHT', [0.5] * len(ss_bm_df))\n",
    "ss_bm_df['BOARDMEMBERS_INFO'] = [{k: v for k, v in x.items() if v == v} for x in ss_bm_df.to_dict('records')]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_bm_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_curr_bm_df = ss_bm_df[ss_bm_df['END_DATE'].isna() == True].loc[:, [\"COMPANY_ID\", \"BOARDMEMBERS_INFO\", \"PERSON_ID\"]]\n",
    "ss_fmr_bm_df = ss_bm_df[ss_bm_df['END_DATE'].isna() == False].loc[:, [\"COMPANY_ID\", \"BOARDMEMBERS_INFO\", \"PERSON_ID\"]]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_fmr_bm_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ss_curr_bm = np.array_split(ss_curr_bm_df, BATCHES)\n",
    "for chunk_df in ss_curr_bm:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"CURRENT_BOARDMEMBER\"\n",
    "start_node_key = (\"Company\", \"SOURCE_ID\")\n",
    "end_node_key = (\"Person\", \"SOURCE_ID\"))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_fmr_bm = np.array_split(ss_fmr_bm_df, BATCHES)\n",
    "for chunk_df in ss_fmr_bm:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"FORMER_BOARDMEMBER\"\n",
    "start_node_key = (\"Company\", \"SOURCE_ID\")\n",
    "end_node_key = (\"Person\", \"SOURCE_ID\"))\n",
    "\n",
    "\n",
    "# ### SourceCrub Executives\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_exec_df = cursor.execute(\n",
    "    'SELECT * FROM AMD_CATALYST_DB.L2_NEW.SOURCESCRUB_EXECUTIVES WHERE LINKEDIN IS NOT NULL').fetch_pandas_all()\n",
    "# ss_exec_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_exec_df['NAME'] = ss_exec_df['FIRSTNAME'] + ' ' + ss_exec_df['LASTNAME']\n",
    "ss_exec_df.drop(columns=['FIRSTNAME', 'LASTNAME'], inplace=True)\n",
    "\n",
    "ss_exec_df['LINKEDIN'] = ss_exec_df['LINKEDIN'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '').lstrip('www.') + str(urlparse(x).path or '')))\n",
    "\n",
    "ss_exec_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "ss_exec_df.rename(columns={'PERSONID': 'PERSON_ID', 'COMPANYID': 'COMPANY_ID', 'STARTDATE': 'START_DATE',\n",
    "                           'ENDDATE': 'END_DATE'}, inplace=True)\n",
    "\n",
    "ss_exec_df.START_DATE = ss_exec_df.START_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "ss_exec_df.END_DATE = ss_exec_df.END_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_exec_df.head(3)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_exec_df.insert(2, 'WEIGHT', [0.5] * len(ss_exec_df))\n",
    "ss_exec_df['EXECUTIVE_INFO'] = [{k: v for k, v in x.items() if v == v} for x in ss_exec_df.to_dict('records')]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_curr_exec_df = ss_exec_df[ss_exec_df['END_DATE'].isna() == True].loc[:,\n",
    "                  [\"COMPANY_ID\", \"EXECUTIVE_INFO\", \"PERSON_ID\"]]\n",
    "ss_fmr_exec_df = ss_exec_df[ss_exec_df['END_DATE'].isna() == False].loc[:,\n",
    "                 [\"COMPANY_ID\", \"EXECUTIVE_INFO\", \"PERSON_ID\"]]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_curr_exec_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_curr_exec = np.array_split(ss_curr_exec_df, BATCHES)\n",
    "for chunk_df in ss_curr_exec:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"CURRENT_EXECUTIVE\"\n",
    "start_node_key = (\"Company\", \"SOURCE_ID\")\n",
    "end_node_key = (\"Person\", \"SOURCE_ID\"))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "ss_fmr_exec = np.array_split(ss_fmr_exec_df, BATCHES)\n",
    "for chunk_df in ss_fmr_exec:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"FORMER_EXECUTIVE\"\n",
    "start_node_key = (\"Company\", \"SOURCE_ID\")\n",
    "end_node_key = (\"Person\", \"SOURCE_ID\"))"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "LShYubDLFPO5bRk3dHX60D",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ### Crunchbase People\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "cb_persons_df = cursor.execute(\n",
    "    'SELECT \"CB URL\", CITY, \"COUNTRY CODE\", \"STATE CODE\", \"FACEBOOK URL\", GENDER, \"LINKEDIN URL\", NAME, \"TWITTER URL\", UUID, \"WEBSITE URL\" FROM L2.CRUNCHBASE_PEOPLE_TWO').fetch_pandas_all()\n",
    "cb_persons_df.head()\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "cb_persons_df.rename(columns={\n",
    "    'UUID': 'PERSON_ID',\n",
    "    'TWITTER URL': 'TWITTER',\n",
    "    'FACEBOOK URL': 'FACEBOOK',\n",
    "    'LINKEDIN URL': 'LINKEDIN',\n",
    "    'CB URL': 'CRUNCHBASE',\n",
    "    'WEBSITE URL': 'WEBSITE',\n",
    "    'STATE CODE': 'STATE',\n",
    "    'COUNTRY CODE': 'COUNTRY', }, inplace=True)\n",
    "\n",
    "cb_persons_df['LOCATION'] = cb_persons_df['CITY'] + ', ' + cb_persons_df['COUNTRY'] + ', ' + \\\n",
    "                            cb_persons_df['STATE']\n",
    "cb_persons_df.drop(columns=['CITY', 'COUNTRY', 'STATE'], inplace=True)\n",
    "\n",
    "cb_persons_df.replace({None: ''}, inplace=True)\n",
    "\n",
    "cb_persons_df['PERSON_ID'] = 'crunchbase-' + cb_persons_df['PERSON_ID'].astype(str)\n",
    "\n",
    "cb_persons_df['LINKEDIN'] = cb_persons_df['LINKEDIN'].apply(lambda x: (\n",
    "        str(urlparse(x.replace('[', '').replace(']', '')).netloc or '') + str(\n",
    "    urlparse(x.replace('[', '').replace(']', '')).path or '')).lstrip('www.'))\n",
    "cb_persons_df['TWITTER'] = cb_persons_df['TWITTER'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.'))\n",
    "cb_persons_df['FACEBOOK'] = cb_persons_df['FACEBOOK'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.'))\n",
    "cb_persons_df['CRUNCHBASE'] = cb_persons_df['CRUNCHBASE'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.'))\n",
    "cb_persons_df['WEBSITE'] = cb_persons_df['WEBSITE'].apply(\n",
    "    lambda x: (str(urlparse(x).netloc or '') + str(urlparse(x).path or '')).lstrip('www.'))\n",
    "\n",
    "cb_persons_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "display(cb_persons_df.head())\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "cb_persons_df.rename(columns={'PERSON_ID': 'SOURCE_ID'}, inplace=True)\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "cb_persons_df_website = np.array_split(\n",
    "    cb_persons_df[cb_persons_df['WEBSITE'].isna() == False][['WEBSITE']].rename(\n",
    "        columns={'WEBSITE': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in cb_persons_df_website:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"DomainURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "cb_persons_df_linkedin = np.array_split(\n",
    "    cb_persons_df[cb_persons_df['LINKEDIN'].isna() == False][['LINKEDIN']].rename(\n",
    "        columns={'LINKEDIN': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in cb_persons_df_linkedin:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"LinkedInURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_persons_df_twitter = np.array_split(\n",
    "    cb_persons_df[cb_persons_df['TWITTER'].isna() == False][['TWITTER']].rename(\n",
    "        columns={'TWITTER': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in cb_persons_df_twitter:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"TwitterURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_persons_df_facebook = np.array_split(\n",
    "    cb_persons_df[cb_persons_df['FACEBOOK'].isna() == False][['FACEBOOK']].rename(\n",
    "        columns={'FACEBOOK': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in cb_persons_df_facebook:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"FacebookURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_persons_df_crunchbase = np.array_split(\n",
    "    cb_persons_df[cb_persons_df['CRUNCHBASE'].isna() == False][['CRUNCHBASE']].rename(\n",
    "        columns={'CRUNCHBASE': 'URL'}).to_dict('records'), BATCHES)\n",
    "for chunk_df in cb_persons_df_crunchbase:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"URLIdentifier\", \"CrunchbaseURL\"), \"URL\"),\n",
    "    )\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "# creating people node\n",
    "cb_persons = np.array_split(\n",
    "    [{k: v for k, v in x.items() if v == v} for x in cb_persons_df.to_dict('records')], BATCHES)\n",
    "for chunk_df in cb_persons:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        ((\"Person\", \"CrunchbasePerson\"), \"SOURCE_ID\"),\n",
    "    )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "for identifier in ['WEBSITE', 'LINKEDIN', 'TWITTER', 'FACEBOOK', 'CRUNCHBASE']:\n",
    "    crunchbase_identifier_relationships = cb_persons.loc[cb_persons[identifier] != np.nan][\n",
    "        ['SOURCE_ID', identifier]]\n",
    "    crunchbase_identifier_relationships.insert(1, 'WEIGHT',\n",
    "                                               [[0.5]] * len(crunchbase_identifier_relationships))\n",
    "\n",
    "    df_list = np.array_split(crunchbase_identifier_relationships, BATCHES)\n",
    "    for chunk_df in df_list:\n",
    "        merge_relationships(\n",
    "            graph.auto(),\n",
    "            chunk_df.values.tolist(),\n",
    "            merge_key=\"LINKED_TO\",\n",
    "            start_node_key=(\"Person\", \"SOURCE_ID\"),\n",
    "            end_node_key=(\"URLIdentifier\", \"URL\"),\n",
    "            keys=[\"WEIGHT\"],\n",
    "        )\n",
    "\n",
    "# ### CrunchBase Person to Company\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "cb_p2c_df = cursor.execute(\n",
    "    'SELECT \"JOB TYPE\", \"TITLE\", \"PERSON UUID\", \"PERSON NAME\", \"ORG NAME\", \"ORG UUID\", \"STARTED ON\", \"ENDED ON\" FROM L2.GRAPH_CRUNCHBASE_CURRENT_JOBS').fetch_pandas_all()\n",
    "# cb_p2c_df.head()\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "np.sum(cb_p2c_df['ENDED ON'] == None)\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "cb_p2c_df.rename(columns=\n",
    "                 {'JOB TYPE': 'JOB_TYPE',\n",
    "                  'PERSON UUID': 'PERSON_ID',\n",
    "                  'PERSON NAME': 'PERSON_NAME',\n",
    "                  'ORG NAME': 'COMPANY_NAME',\n",
    "                  'ORG UUID': 'COMPANY_ID',\n",
    "                  'STARTED ON': 'START_DATE',\n",
    "                  'ENDED ON': 'END_DATE'}, inplace=True)\n",
    "\n",
    "#cb_p2c_df.replace({None: ''}, inplace=True)\n",
    "\n",
    "cb_p2c_df['COMPANY_ID'] = 'crunchbase-' + cb_p2c_df['COMPANY_ID'].astype(str)\n",
    "cb_p2c_df['PERSON_ID'] = 'crunchbase-' + cb_p2c_df['PERSON_ID'].astype(str)\n",
    "cb_p2c_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "cb_p2c_df.START_DATE = cb_p2c_df.START_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "cb_p2c_df.END_DATE = cb_p2c_df.END_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "cb_p2c_df.insert(2, 'WEIGHT', [0.5] * len(cb_p2c_df))\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "cb_p2c_df.head()\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "# cb_p2c_df.drop(columns = \"WEIGHT\", inplace = True)\n",
    "# add statement like if Weight not exist add weight\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "cb_p2c_curr_df = cb_p2c_df[cb_p2c_df['END_DATE'].isna()]\n",
    "cb_p2c_fmr_df = cb_p2c_df[cb_p2c_df['END_DATE'].isna() == False]\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "cb_p2c_curr_df['CURRENT_EMP_DETAILS'] = [{k: v for k, v in x.items() if v == v} for x in\n",
    "                                         cb_p2c_curr_df.to_dict('records')]\n",
    "cb_curr_emp_df = cb_p2c_curr_df[[\"COMPANY_ID\", \"CURRENT_EMP_DETAILS\", \"PERSON_ID\"]]\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "cb_curr_emp_df.head()\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "cb_fmr_emp_df.head()\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "# cb_fmr_emp_df.head()\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "cb_curr_emp = np.array_split(cb_curr_emp_df, BATCHES)\n",
    "for chunk_df in cb_curr_emp:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"CURRENT_EMPLOYMENT\",\n",
    "        start_node_key=(\"Company\", \"SOURCE_ID\"),\n",
    "        end_node_key=(\"Person\", \"SOURCE_ID\"))\n",
    "\n",
    "\n",
    "cb_event_appear_df = cursor.execute(\n",
    "    'SELECT * FROM L2_NEW.GRAPH_CRUNCHBASE_EVENT_PARTICIPANTS_PERSONS').fetch_pandas_all()\n",
    "cb_event_appear_df.head()\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "print(\"Is Unique: \", cb_event_appear_df.EVENT_UUID.is_unique)\n",
    "\n",
    "# In[164]:\n",
    "\n",
    "cb_event_appear_df.rename(columns={\n",
    "    'PARTICIPANT_UUID': 'PERSON_ID',\n",
    "    \"APPEARANCE_TYPE\": \"TYPE\",\n",
    "    \"EVENT_UUID\": \"EVENT_ID\",\n",
    "    'PARTICIPANT NAME': 'PERSON_NAME'}, inplace=True)\n",
    "\n",
    "cb_event_appear_df['PERSON_ID'] = 'crunchbase-' + cb_event_appear_df['PERSON_ID'].astype(str)\n",
    "cb_event_appear_df['EVENT_ID'] = 'crunchbase-' + cb_event_appear_df['EVENT_ID'].astype(str)\n",
    "\n",
    "cb_event_appear_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "cb_event_appear_df.head()\n",
    "\n",
    "# In[166]:\n",
    "\n",
    "if 'WEIGHT' not in list(cb_event_appear_df.columns):\n",
    "    cb_event_appear_df.insert(0, 'WEIGHT', [0.5] * len(cb_event_appear_df))\n",
    "\n",
    "# In[167]:\n",
    "\n",
    "cb_event_appear_df['PARTICIPANT_INFO'] = [{k: v for k, v in x.items() if v == v} for x in\n",
    "                                          cb_event_appear_df.to_dict('records')]\n",
    "\n",
    "# In[169]:\n",
    "\n",
    "cb_event_appear_df['PARTICIPANT_INFO'] = [{k: v for k, v in x.items() if v == v} for x in\n",
    "                                          cb_event_appear_df.to_dict('records')]\n",
    "cb_event_appear_df_final = cb_event_appear_df[[\"PERSON_ID\", \"PARTICIPANT_INFO\", \"EVENT_ID\"]]\n",
    "\n",
    "# In[171]:\n",
    "\n",
    "cb_event_appear_df_final.head()\n",
    "\n",
    "# In[213]:\n",
    "\n",
    "cb_event_appear = np.array_split(cb_event_appear_df_final, BATCHES)\n",
    "\n",
    "for chunk_df in cb_event_appear:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"ATTEND_EVENT\",\n",
    "        start_node_key=(\"Person\", \"SOURCE_ID\"),\n",
    "        end_node_key=(\"Event\", \"SOURCE_ID\"))\n",
    "\n",
    "# ### CrunchBase Degree\n",
    "# ##### Creating the school node\n",
    "\n",
    "# In[245]:\n",
    "\n",
    "univ_df = cursor.execute(\n",
    "    'SELECT \"INSTITUTION NAME\", \"INSTITUTION UUID\" FROM L2.GRAPH_CRUNCHBASE_DEGREES').fetch_pandas_all()\n",
    "univ_df.rename(columns={\n",
    "    'INSTITUTION NAME': 'SCHOOL_NAME',\n",
    "    'INSTITUTION UUID': 'SOURCE_ID'}, inplace=True)\n",
    "\n",
    "univs = np.array_split([{k: v for k, v in x.items() if v == v} for x in univ_df.to_dict('records')],\n",
    "                       BATCHES)\n",
    "\n",
    "# In[247]:\n",
    "\n",
    "for chunk_df in univs:\n",
    "    merge_nodes(\n",
    "        graph.auto(),\n",
    "        chunk_df,\n",
    "        (\"School\", \"SOURCE_ID\"),\n",
    "    )\n",
    "\n",
    "# ##### Creating relationship\n",
    "\n",
    "# In[256]:\n",
    "\n",
    "cb_degrees_df = cursor.execute(\n",
    "    'SELECT \"CB URL\", \"DEGREE TYPE\", \"PERSON NAME\", \"PERSON UUID\", \"SUBJECT\", \"INSTITUTION NAME\", \"INSTITUTION UUID\", \"STARTED ON\", \"COMPLETED ON\" FROM L2.GRAPH_CRUNCHBASE_DEGREES').fetch_pandas_all()\n",
    "cb_degrees_df.head()\n",
    "\n",
    "# In[257]:\n",
    "\n",
    "cb_degrees_df.rename(columns={\n",
    "    'PERSON UUID': 'PERSON_ID',\n",
    "    \"CB URL\": \"CRUNCHBASE\",\n",
    "    'PERSON NAME': 'PERSON_NAME',\n",
    "    'DEGREE TYPE': 'DEGREE',\n",
    "    'INSTITUTION NAME': 'SCHOOL_NAME',\n",
    "    'INSTITUTION UUID': 'SCHOOL_ID',\n",
    "    'STARTED ON': 'START_DATE',\n",
    "    'COMPLETED ON': 'END_DATE'}, inplace=True)\n",
    "\n",
    "cb_degrees_df['PERSON_ID'] = 'crunchbase-' + cb_degrees_df['PERSON_ID'].astype(str)\n",
    "cb_degrees_df['SCHOOL_ID'] = 'crunchbase-' + cb_degrees_df['SCHOOL_ID'].astype(str)\n",
    "\n",
    "cb_degrees_df.START_DATE = cb_degrees_df.START_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "cb_degrees_df.END_DATE = cb_degrees_df.END_DATE.fillna(0).astype('str').replace('0', np.nan)\n",
    "cb_degrees_df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# In[258]:\n",
    "\n",
    "if 'WEIGHT' not in list(cb_degrees_df.columns):\n",
    "    cb_degrees_df.head().insert(0, 'WEIGHT', [0.5] * len(cb_degrees_df.head()))\n",
    "cb_degrees_df['EDUCATION_HISTORY'] = [{k: v for k, v in x.items() if v == v} for x in\n",
    "                                      cb_degrees_df.to_dict('records')]\n",
    "cb_degrees_df.head()\n",
    "\n",
    "# In[264]:\n",
    "\n",
    "cb_degrees_df.rename(columns={\n",
    "    'PERSON_ID': 'SOURCE_ID'}, inplace=True)\n",
    "cb_degrees_df.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "for identifier in ['CRUNCHBASE']:\n",
    "    cb_identifier_relationships = cb_degrees_df.loc[cb_degrees_df[identifier] != np.nan][\n",
    "        ['SOURCE_ID', identifier]]\n",
    "    cb_identifier_relationships.insert(1, 'WEIGHT', [0.5] * len(cb_identifier_relationships))\n",
    "\n",
    "    df_list = np.array_split(cb_identifier_relationships, BATCHES)\n",
    "    for chunk_df in df_list:\n",
    "        merge_relationships(\n",
    "            graph.auto(),\n",
    "            chunk_df.values.tolist(),\n",
    "            merge_key=\"LINKED_TO\",\n",
    "            start_node_key=(\"Person\", \"SOURCE_ID\"),\n",
    "            end_node_key=(\"URLIdentifier\", \"URL\"),\n",
    "            keys=[\"WEIGHT\"],\n",
    "        )\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_degrees_df_final = cb_degrees_df[['PERSON_ID', 'EDUCATION_HISTORY', 'SCHOOL_ID']]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_degrees_df_final.head()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cb_degree = np.array_split(cb_degrees_df_final, BATCHES)\n",
    "\n",
    "for chunk_df in cb_degrees:\n",
    "    merge_relationships(\n",
    "        graph.auto(),\n",
    "        chunk_df.values.tolist(),\n",
    "        merge_key=\"ATTEND_SCHOOL\",\n",
    "        start_node_key=(\"Person\", \"SOURCE_ID\"),\n",
    "        end_node_key=(\"School\", \"SOURCE_ID\"))\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "P2EL15ezn1mpjm38zB0dww",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "datalore": {
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "base_environment": "default",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
